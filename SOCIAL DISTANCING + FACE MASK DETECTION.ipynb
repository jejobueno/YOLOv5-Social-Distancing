{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c592a110",
   "metadata": {},
   "source": [
    "# SOCIAL DITANCING AND FACE MASK DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc5074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cvzone\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from cvzone.FaceMeshModule import FaceMeshDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6994b3",
   "metadata": {},
   "source": [
    "## 1.1 FUNCTION TO CALCULATE THE DISTANCE FROM THE CAMERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5588b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "def detect_distance_from_camera(img, detector):\n",
    "    #detect the faceMesh\n",
    "    img, faces = detector.findFaceMesh(img, draw=False)\n",
    "    \n",
    "    distances = list()\n",
    "    \n",
    "    if faces:\n",
    "        for face in faces:\n",
    "            pointLeft = face[145]\n",
    "            pointRight = face[374]\n",
    "            \n",
    "            color = (0, 200, 0)\n",
    "            #cv2.line(img, pointLeft, pointRight, color, 3)\n",
    "            #cv2.circle(img, pointLeft, 5, (255,0,255), cv2.FILLED)\n",
    "            #cv2.circle(img, pointRight, 5, (255,0,255), cv2.FILLED)\n",
    "            \n",
    "            w = detector.findDistance(pointLeft, pointRight)[0]\n",
    "            #cv2.putText(img, str(w[0]), (pointLeft), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            \n",
    "            # Trying to find the deep of the focal lent\n",
    "            W = 6.3 #avergae distance between the eyes\n",
    "            #d = 30\n",
    "            #f = (w*d)/W\n",
    "            f = 540\n",
    "            #cv2.putText(img, str(f), (pointLeft),cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,0), 2)\n",
    "            \n",
    "            # Finding the distance from the focal point\n",
    "            d = (W*f)/w\n",
    "            \n",
    "            # Create a list with the coordinates of the eyes and the distance\n",
    "            distances.append([d, pointLeft, pointRight])\n",
    "            #cv2.putText(img, \"{:.2f} cm\".format(d), (face[10]),cv2.FONT_HERSHEY_SIMPLEX, 2/int(d*0.047), (0,0,0), 2)\n",
    "            \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f0cf2",
   "metadata": {},
   "source": [
    "## 1.2 Person detection with opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026cfd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading person detector model...\n"
     ]
    }
   ],
   "source": [
    "# load our serialized person detector model from disk\n",
    "print(\"[INFO] loading person detector model...\")\n",
    "prototxtPath = os.path.sep.join([\"person_detector\", \"SSD_MobileNet_prototxt.txt\"])\n",
    "weightsPath = os.path.sep.join([\"person_detector\",\n",
    "                                \"SSD_MobileNet.caffemodel\"])\n",
    "net_person = cv2.dnn.readNet(prototxtPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40033ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Status] Loading Model...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import imutils\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from imutils.video import FPS\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "#Initialize Objects and corresponding colors which the model can detect\n",
    "labels = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \n",
    "\"boat\",\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \n",
    "\"diningtable\",\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \n",
    "\"sheep\",\"sofa\", \"train\", \"tvmonitor\"]\n",
    "colors = np.random.uniform(0, 255, size=(len(labels), 3))\n",
    "\n",
    "#Loading Caffe Model\n",
    "print('[Status] Loading Model...')\n",
    "prototxtPath = os.path.sep.join([\"person_detector\", \"SSD_MobileNet_prototxt.txt\"])\n",
    "weightsPath = os.path.sep.join([\"person_detector\",\n",
    "                                \"SSD_MobileNet.caffemodel\"])\n",
    "nn = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "img = cv2.imread('images/pic2.jpg')\n",
    "\n",
    "img = imutils.resize(img, width=400)\n",
    "(h, w) = img.shape[:2]\n",
    "\n",
    "#Converting Frame to Blob\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), \n",
    "    0.007843, (300, 300), 127.5)\n",
    "\n",
    "#Passing Blob through network to detect and predict\n",
    "nn.setInput(blob)\n",
    "detections = nn.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a38b4c1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46757/2441856009.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstartX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Loop over the detections\n",
    "for i in np.arange(0, detections.shape[2]):\n",
    "\n",
    "#Extracting the confidence of predictions\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    #Filtering out weak predictions\n",
    "    if confidence > 0.7:\n",
    "            \n",
    "        #Extracting the index of the labels from the detection\n",
    "        #Computing the (x,y) - coordinates of the bounding box        \n",
    "        idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "        #Extracting bounding box coordinates\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "        #Drawing the prediction and bounding box\n",
    "        label = \"{}: {:.2f}%\".format(labels[idx], confidence * 100)\n",
    "        cv2.rectangle(img, (startX, startY), (endX, endY), colors[idx], 2)\n",
    "\n",
    "        y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "        cv2.putText(img, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[idx], 2)\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63d9f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Status] Starting Video Stream...\n",
      "[Info] Elapsed time: 3.10\n",
      "[Info] Approximate FPS:  24.22\n"
     ]
    }
   ],
   "source": [
    "#Initialize Video Stream\n",
    "print('[Status] Starting Video Stream...')\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "\n",
    "#Loop Video Stream\n",
    "while True:\n",
    "\n",
    "    #Resize Frame to 400 pixels\n",
    "    frame = vs.read()\n",
    "    frame = imutils.resize(frame, width=400)\n",
    "    (h, w) = frame.shape[:2]\n",
    "\n",
    "    #Converting Frame to Blob\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), \n",
    "    \t0.007843, (300, 300), 127.5)\n",
    "\n",
    "    #Passing Blob through network to detect and predict\n",
    "    nn.setInput(blob)\n",
    "    detections = nn.forward()\n",
    "\n",
    "\n",
    "    #Loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "\n",
    "\t#Extracting the confidence of predictions\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        #Filtering out weak predictions\n",
    "        if confidence > 0.7:\n",
    "            \n",
    "            #Extracting the index of the labels from the detection\n",
    "            #Computing the (x,y) - coordinates of the bounding box        \n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "            #Extracting bounding box coordinates\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            #Drawing the prediction and bounding box\n",
    "            label = \"{}: {:.2f}%\".format(labels[idx], confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), colors[idx], 2)\n",
    "\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[idx], 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    \n",
    "    fps.update()\n",
    "\n",
    "fps.stop()\n",
    "\n",
    "print(\"[Info] Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[Info] Approximate FPS:  {:.2f}\".format(fps.fps()))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f3aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf388dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno -3] Temporary failure in name resolution>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1353\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1250\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    923\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46486/2771532839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s-fp16.tflite')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0myolo_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ultralytics/yolov5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yolov5s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meuclidean_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'github'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mrepo_or_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_cache_or_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_validation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_get_cache_or_reload\u001b[0;34m(github, force_reload, verbose, skip_validation)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m# Parse github repo information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mrepo_owner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_repo_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgithub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Github allows branch name with slash '/',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# this causes confusion with path on both Linux and Windows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36m_parse_repo_info\u001b[0;34m(github)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m# then it's the default branch, otherwise it's master.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"https://github.com/{repo_owner}/{repo_name}/tree/main/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mbranch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1397\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1398\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s-fp16.tflite')\n",
    "yolo_model = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\") \n",
    "\n",
    "def euclidean_dist(p1, p2):\n",
    "    \"\"\"\n",
    "    p1, p2 = two points for calculating Euclidean Distance\n",
    "\n",
    "    :return:\n",
    "    dst = Euclidean Distance between two 2d points\n",
    "    \"\"\"\n",
    "    if p1 is int:\n",
    "        dst = math.sqrt(p1 ** 2 + p2 ** 2)\n",
    "        print(type(p1))\n",
    "    else:\n",
    "        dst = math.sqrt((p2[0] - p1[0] ) ** 2 + (p2[1] - p1[1])** 2)\n",
    "    \n",
    "    return dst\n",
    "        \n",
    "\n",
    "\n",
    "def detect_sd(frame, distances):\n",
    "    detections = yolo_model(frame, size=640)\n",
    "    detections = detections.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "    print(detections)\n",
    "\n",
    "    if len(detections) > 0:  # At least 1 detection in the image and check detection presence in a frame\n",
    "        centroid_dict = dict()  # Function creates a dictionary and calls it centroid_dict\n",
    "        distances_dict = dict()\n",
    "        objectId = 0  # We inialize a variable called ObjectId and set it to 0\n",
    "        for detection in detections:  # In this if statement, we filter all the detections for persons only\n",
    "            # Check for the only person name tag \n",
    "            name_tag = detection['name']  # Coco file has string of all the names\n",
    "            if name_tag == 'person':\n",
    "                xmin, ymin, xmax, ymax = detection['xmin'], detection['ymin'], detection['xmax'], detection[\n",
    "                    'ymax']  # Store the center points of the detections\n",
    "                x = int(xmin + (xmax - xmin) / 2)\n",
    "                y = int(ymin + (ymax - ymin) / 2)\n",
    "                centroid_dict[objectId] = (x, y, int(xmin), int(ymin), int(xmax),\n",
    "                                           int(ymax))  # Create dictionary of tuple with 'objectId' as the index center points and bbox\n",
    "                #[for distance in distances]\n",
    "                distances_dict[objectId] = [distance for distance in distances if xmin < distance[1][0] < xmax and ymin < distance[1][0] < ymax]\n",
    "                objectId += 1  # Increment the index for each detection\n",
    "\n",
    "\n",
    "        # =================================================================#\n",
    "\n",
    "        # =================================================================\n",
    "        # 3.2 Purpose : Determine which person bbox are close to each other\n",
    "        # =================================================================\n",
    "        print('Eye distances: ', distances_dict)\n",
    "        print('Persons recogized: ', centroid_dict)\n",
    "        red_zone_list = []  # List containing which Object id is in under threshold distance condition.\n",
    "        red_line_list = []\n",
    "        for (id1, p1), (id2, p2) in combinations(centroid_dict.items(),2):  # Get all the combinations of close detections, #List of multiple items - id1 1, points 2, 1,3\n",
    "            dx, dy = p1[0] - p2[0], p1[1] - p2[1]  # Check the difference between centroid x: 0, y :1\n",
    "            socialdist_px = euclidean_dist(p1[0:2], p2[0:2])  # Calculates the Euclidean distance\n",
    "                \n",
    "            socialdist_px = euclidean_dist(p1, p2)\n",
    "            distance_bepx1 = 0\n",
    "            distance_bepx2 = 0\n",
    "            no_eye_ref = False\n",
    "            \n",
    "            distance_fc1 = 0\n",
    "            distance_fc2 = 0\n",
    "            \n",
    "            if id1 in distances:\n",
    "                if len(distances[id1]) > 0:\n",
    "                    distance_bepx1 = euclidean_dist(distances[id1][0][1], distances[id1][0][2])\n",
    "                    distance_fc1 = distances[id1][0][0]\n",
    "            if id2 in distances:\n",
    "                if len(distances[id2]) > 0:\n",
    "                    distance_bepx2 = euclidean_dist(distances[id2][0][1], distances[id2][0][2])\n",
    "                    distance_fc1 = distances[id2][0][0]\n",
    "            \n",
    "            if distance_bepx1 > distance_bepx2:\n",
    "                socialdist = socialdist_px*6.3/distance_bepx1                    \n",
    "            elif distance_bepx1 < distance_bepx2:\n",
    "                socialdist = socialdist_px*6.3/distance_bepx2\n",
    "            else:\n",
    "                socialdist = socialdist_px\n",
    "                no_eye_ref = True\n",
    "                \n",
    "            if distance_fc1 > 0 and distance_fc2 > 0:\n",
    "                if distance_fc2 > distance_fc1:\n",
    "                    socialdist = math.sqrt((distance_fc2-distance_fc1)**2 + (socialdist)**2)\n",
    "            \n",
    "            ### Referencing horizontal distances to between eyes distance (6.3cm)\n",
    "            if (socialdist < 150.0 and not no_eye_ref) or (socialdist < 300 and no_eye_ref):  # Set our social distance threshold - If they meet this condition then..\n",
    "                if id1 not in red_zone_list:\n",
    "                    red_zone_list.append(id1)  # Add Id to a list\n",
    "                    red_line_list.append(p1[0:2])  # Add points to the list\n",
    "                    #red_zone_dists.append(socialdist)\n",
    "                if id2 not in red_zone_list:\n",
    "                    red_zone_list.append(id2)  # Same for the second id\n",
    "                    red_line_list.append(p2[0:2])\n",
    "                ###############\n",
    "                # Print the distance horizontal in cm\n",
    "                ##############\n",
    "                cv2.line(frame, p1[0:2], p2[0:2], (0, 0, 255), 1)\n",
    "                if not no_eye_ref:\n",
    "                    cv2.putText(frame, \"{:.2f} cm\".format(socialdist),(p1[2],p1[3]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)  # Display Text\n",
    "                ################\n",
    "            print(p1, '####', p2, '####', socialdist)\n",
    "        for idx, box in centroid_dict.items():  # dict (1(key):red(value), 2 blue)  idx - key  box - value\n",
    "            if idx in red_zone_list:  # if id is in red zone list\n",
    "                #cv2.putText(frame, \"{:.2f} cm\".format(socialdist),(box[2], box[3]),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "                cv2.rectangle(frame, (box[2], box[3]), (box[4], box[5]), (0, 0, 255),\n",
    "                              2)  # Create Red bounding boxes  #starting point, ending point size of 2\n",
    "            else:\n",
    "                cv2.rectangle(frame, (box[2], box[3]), (box[4], box[5]), (0, 255, 0),\n",
    "                              2)  # Create Green bounding boxe bounding boxes  #starting\n",
    "        # =================================================================\n",
    "        # 3.3 Purpose : Display Risk Analytics and Show Risk Indicators\n",
    "        # =================================================================\n",
    "        text = \"People at Risk: %s\" % str(len(red_zone_list))  # Count People at Risk\n",
    "        location = (10, 25)  # Set the location of the displayed text\n",
    "        cv2.putText(frame, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1,\n",
    "                    cv2.LINE_AA)  # Display Text\n",
    "\n",
    "        for check in range(0,\n",
    "                           len(red_line_list) - 1):  # Draw line between nearby bboxes iterate through redlist items\n",
    "            start_point = red_line_list[check]\n",
    "            end_point = red_line_list[check + 1]\n",
    "            check_line_x = abs(end_point[0] - start_point[0])  # Calculate the line coordinates for x\n",
    "            check_line_y = abs(end_point[1] - start_point[1])  # Calculate the line coordinates for y\n",
    "            if (check_line_x < 75) and (\n",
    "                    check_line_y < 25):  # If both are We check that the lines are below our threshold distance.\n",
    "                cv2.line(frame, start_point, end_point, (255, 0, 0),\n",
    "                         2)  # Only above the threshold lines are displayed.\n",
    "    # =================================================================#\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74366804",
   "metadata": {},
   "source": [
    "# Face mask detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77be0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 23:08:17.208726: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jejobueno/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-11 23:08:17.208752: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading face detector model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 23:08:18.397251: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jejobueno/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-02-11 23:08:18.397283: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-11 23:08:18.397308: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jejobueno-HP-EliteBook-840-G2): /proc/driver/nvidia/version does not exist\n",
      "2022-02-11 23:08:18.397527: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-11 23:08:18.420737: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2593980000 Hz\n",
      "2022-02-11 23:08:18.421195: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7b6a920 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-02-11 23:08:18.421231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# load our serialized face detector model from disk\n",
    "print(\"[INFO] loading face detector model...\")\n",
    "prototxtPath = os.path.sep.join([\"face_detector\", \"deploy.prototxt\"])\n",
    "weightsPath = os.path.sep.join([\"face_detector\",\n",
    "                                \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "model = load_model(\"mask_detector.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9373d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(detections, image, w, h):\n",
    "    masks = 0\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the detection\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the confidence is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # ensure the bounding boxes fall within the dimensions of\n",
    "            # the frame\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
    "\n",
    "            # extract the face ROI, convert it from BGR to RGB channel\n",
    "            # ordering, resize it to 224x224, and preprocess it\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face = cv2.resize(face, (224, 224))\n",
    "            face = img_to_array(face)\n",
    "            face = preprocess_input(face)\n",
    "            face = np.expand_dims(face, axis=0)\n",
    "\n",
    "            # pass the face through the model to determine if the face\n",
    "            # has a mask or not\n",
    "            (mask, withoutMask) = model.predict(face)[0]\n",
    "\n",
    "            # determine the class label and color we'll use to draw\n",
    "            # the bounding box and text\n",
    "            label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "            masks = masks + 0 if mask > withoutMask else masks + 1\n",
    "            color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "\n",
    "            # include the probability in the label\n",
    "            label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "\n",
    "            # display the label and bounding box rectangle on the output\n",
    "            # frame\n",
    "            cv2.putText(image, label, (startX, startY - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
    "            \n",
    "    text = \"People awithout mask: %s\" % str(masks)  # Count People at Risk\n",
    "    location = (10, 50)  # Set the location of the displayed text\n",
    "    cv2.putText(frame, text, location, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1,\n",
    "            cv2.LINE_AA)  # Display Text\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def detect(image):\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # construct a blob from the image\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
    "                                 (104.0, 177.0, 123.0))\n",
    "\n",
    "    # pass the blob through the network and obtain the face detections\n",
    "    #print(\"[INFO] computing face detections...\")\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    \n",
    "    #drawing boundry boxes\n",
    "    detected_image = draw_detections(detections, image, w, h)\n",
    "    return detected_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532bd5b6",
   "metadata": {},
   "source": [
    "# Detect with camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a4f40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_46486/3537944629.py\", line 27, in <module>\n",
      "    frame = detect_sd(frame, distances)\n",
      "NameError: name 'detect_sd' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import traceback\n",
    "\n",
    "import cv2\n",
    "import cvzone\n",
    "import numpy as np\n",
    "from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = FaceMeshDetector()\n",
    "cv2.namedWindow(\"Image\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        #########################################\n",
    "        # Face and distance from screen detection\n",
    "        #########################################\n",
    "\n",
    "        distances = detect_distance_from_camera(frame, detector)\n",
    "\n",
    "        #######################################\n",
    "        #    People detection:\n",
    "        #######################################\n",
    "\n",
    "        frame = detect_sd(frame, distances)\n",
    "        \n",
    "        #######################################\n",
    "        #    Face Mask Detection\n",
    "        ########################################\n",
    "        \n",
    "        # Draw the detected objects\n",
    "        detected_frame = detect(frame)\n",
    "        detected_frame = cv2.cvtColor(detected_frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        ########################################\n",
    "        # Show on screen\n",
    "        ########################################\n",
    "        cv2.resizeWindow('Image', 600,600)\n",
    "        cv2.imshow(\"Image\", detected_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            cap.release()\n",
    "            break\n",
    "except Exception as errors:\n",
    "    errors = io.StringIO()\n",
    "    traceback.print_exc(file=errors)\n",
    "    contents = str(errors.getvalue())\n",
    "    print(contents)\n",
    "    errors.close()\n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2923a3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3fdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facedisttest-env",
   "language": "python",
   "name": "facedisttest-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
